{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community faiss-cpu rank_bm25"
      ],
      "metadata": {
        "id": "wUTCrTM7-S23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAGES_TO_CRAWL = 5\n",
        "TOP_K_RESULTS = 5\n",
        "BASE_URL = \"https://python.langchain.com/\"\n",
        "BASE_DIR = \"built_index\"\n",
        "REQUEST_TIMEOUT = 10\n",
        "EMBEDDINGS_MODEL = \"all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "id": "rFSo6wbnI07F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLD CODE"
      ],
      "metadata": {
        "id": "mHHhIoEIVJLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urldefrag\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import Hugging Face embeddings from LangChain and FAISS vector store\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Import BM25 from rank_bm25 for sparse searching\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    os.makedirs(BASE_DIR)\n",
        "\n",
        "class SearchEngine:\n",
        "    def __init__(self,\n",
        "                 base_url=\"https://python.langchain.com/\",\n",
        "                 index_file=BASE_DIR + \"/\"+ \"search_index.json\",\n",
        "                 vector_store_dir=BASE_DIR + \"/\" + \"vector_store\",\n",
        "                 bm25_index_file=BASE_DIR + \"/\" + \"bm25_index.pkl\"):\n",
        "        self.base_url = base_url\n",
        "        self.index_file = index_file\n",
        "        self.vector_store_dir = vector_store_dir\n",
        "        self.bm25_index_file = bm25_index_file\n",
        "        self.index = defaultdict(dict)\n",
        "        self.visited_urls = set()\n",
        "        self.urls_to_visit = [base_url]\n",
        "        self.load_index()\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        self.vector_store = None\n",
        "        self.bm25 = None      # BM25 object for sparse search\n",
        "        self.bm25_texts = []  # List of tokenized texts\n",
        "        self.url_order = []   # To maintain order corresponding to BM25 texts\n",
        "\n",
        "        # Load semantic vector store if available\n",
        "        if os.path.exists(self.vector_store_dir):\n",
        "            try:\n",
        "                self.vector_store = FAISS.load_local(\n",
        "                    self.vector_store_dir,\n",
        "                    self.embeddings,\n",
        "                    allow_dangerous_deserialization=True\n",
        "                )\n",
        "                print(\"Loaded vector store from disk.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load vector store: {e}\")\n",
        "\n",
        "        # Load BM25 index if available\n",
        "        self.load_bm25_index()\n",
        "\n",
        "    def load_index(self):\n",
        "        if os.path.exists(self.index_file):\n",
        "            with open(self.index_file, 'r') as f:\n",
        "                self.index = defaultdict(dict, json.load(f))\n",
        "\n",
        "    def save_index(self):\n",
        "        with open(self.index_file, 'w') as f:\n",
        "            json.dump(dict(self.index), f)\n",
        "\n",
        "    def load_bm25_index(self):\n",
        "        if os.path.exists(self.bm25_index_file):\n",
        "            try:\n",
        "                with open(self.bm25_index_file, 'rb') as f:\n",
        "                    self.bm25, self.bm25_texts, self.url_order = pickle.load(f)\n",
        "                print(\"Loaded BM25 index from disk.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load BM25 index: {e}\")\n",
        "\n",
        "    def save_bm25_index(self):\n",
        "        try:\n",
        "            with open(self.bm25_index_file, 'wb') as f:\n",
        "                pickle.dump((self.bm25, self.bm25_texts, self.url_order), f)\n",
        "            print(\"BM25 index saved to disk.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save BM25 index: {e}\")\n",
        "\n",
        "    def fetch_page(self, url):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_content(self, html):\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        # Remove unwanted tags\n",
        "        for elem in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
        "            elem.decompose()\n",
        "        main_content = soup.find('main') or soup.find('article') or soup\n",
        "        return main_content.get_text(separator=' ', strip=True)\n",
        "\n",
        "    def process_page(self, url, html):\n",
        "        content = self.extract_content(html)\n",
        "        self.index[url] = {\n",
        "            'content': content,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "    def find_links(self, html, base_url):\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            absolute_url = urljoin(base_url, href)\n",
        "            absolute_url, _ = urldefrag(absolute_url)\n",
        "            if absolute_url.startswith(self.base_url) and absolute_url not in self.visited_urls:\n",
        "                self.urls_to_visit.append(absolute_url)\n",
        "\n",
        "    def crawl(self, max_pages=PAGES_TO_CRAWL):\n",
        "        print(\"Starting indexing process...\")\n",
        "        pbar = tqdm(total=max_pages, desc=\"Pages Crawled\")\n",
        "        while self.urls_to_visit and len(self.visited_urls) < max_pages:\n",
        "            current_url = self.urls_to_visit.pop(0)\n",
        "            if current_url in self.visited_urls:\n",
        "                continue\n",
        "\n",
        "            html = self.fetch_page(current_url)\n",
        "            if html:\n",
        "                self.process_page(current_url, html)\n",
        "                self.find_links(html, current_url)\n",
        "                self.visited_urls.add(current_url)\n",
        "                pbar.update(1)\n",
        "        pbar.close()\n",
        "        self.save_index()\n",
        "        print(f\"Index updated. Total pages: {len(self.index)}\")\n",
        "        self.build_indexes()\n",
        "\n",
        "    def build_indexes(self):\n",
        "        # Build semantic vector store and BM25 index together.\n",
        "        print(\"Building semantic vector store and BM25 sparse index...\")\n",
        "        texts, metadatas = [], []\n",
        "        self.url_order = []  # reset BM25 order list\n",
        "        for url, data in self.index.items():\n",
        "            texts.append(data['content'])\n",
        "            metadatas.append({\"url\": url})\n",
        "            self.url_order.append(url)\n",
        "        # Build semantic index\n",
        "        self.vector_store = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)\n",
        "        self.vector_store.save_local(self.vector_store_dir)\n",
        "        print(\"Semantic vector store built and saved successfully.\")\n",
        "        # Build BM25 index\n",
        "        self.build_bm25_index(texts)\n",
        "\n",
        "    def build_bm25_index(self, texts):\n",
        "        print(\"Building BM25 sparse index...\")\n",
        "        # Tokenize each document (using a simple whitespace split)\n",
        "        self.bm25_texts = [text.lower().split() for text in texts]\n",
        "        self.bm25 = BM25Okapi(self.bm25_texts)\n",
        "        print(\"BM25 index built.\")\n",
        "        self.save_bm25_index()\n",
        "\n",
        "    def bm25_search(self, query, top_k=TOP_K_RESULTS):\n",
        "        if not self.bm25:\n",
        "            print(\"BM25 index is not built. Building BM25 index now...\")\n",
        "            texts = [data['content'] for data in self.index.values()]\n",
        "            self.build_bm25_index(texts)\n",
        "        query_tokens = query.lower().split()\n",
        "        scores = self.bm25.get_scores(query_tokens)\n",
        "        # Get indices of the top k scores\n",
        "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            url = self.url_order[idx] if idx < len(self.url_order) else \"N/A\"\n",
        "            results.append({\n",
        "                \"url\": url,\n",
        "                \"score\": scores[idx],\n",
        "                \"snippet\": self.index[url]['content'][:200] if url in self.index else \"\"\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def semantic_search(self, query, top_k=TOP_K_RESULTS):\n",
        "        if not self.vector_store:\n",
        "            print(\"Semantic vector store is not built. Building now...\")\n",
        "            self.build_indexes()\n",
        "        results = self.vector_store.similarity_search(query, k=top_k)\n",
        "        sem_results = []\n",
        "        for result in results:\n",
        "            url = result.metadata.get(\"url\", \"N/A\")\n",
        "            sem_results.append({\n",
        "                \"url\": url,\n",
        "                \"score\": None,  # semantic search does not return a raw BM25-like score\n",
        "                \"snippet\": result.page_content[:200]\n",
        "            })\n",
        "        return sem_results\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"\n",
        "        method can be 'semantic', 'bm25', or 'combined'\n",
        "        \"\"\"\n",
        "        sem_results = self.semantic_search(query, top_k=top_k)\n",
        "        bm25_results = self.bm25_search(query, top_k=top_k)\n",
        "        return {\"semantic\": sem_results, \"bm25\": bm25_results}\n",
        "\n",
        "    def interactive_search(self):\n",
        "        query = input(\"\\nEnter search query: \").strip()\n",
        "        start_time = time.time()\n",
        "        results = self.search(query)\n",
        "        search_time = time.time() - start_time\n",
        "        print(f\"\\nSemantic Search Results (found {len(results['semantic'])} results in {search_time:.2f}s):\")\n",
        "        for i, res in enumerate(results['semantic'], 1):\n",
        "            print(f\"{i}. {res['url']}\")\n",
        "            print(f\"Snippet: {res['snippet']}...\\n\")\n",
        "        print(f\"BM25 Sparse Search Results:\")\n",
        "        for i, res in enumerate(results['bm25'], 1):\n",
        "            print(f\"{i}. {res['url']} (Score: {res['score']:.2f})\")\n",
        "            print(f\"Snippet: {res['snippet']}...\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    engine = SearchEngine(BASE_URL)\n",
        "\n",
        "    # If the index file exists, ask whether to update/re-crawl the index.\n",
        "    if os.path.exists(engine.index_file) and engine.index:\n",
        "        choice = input(\"Index found. Do you want to update (re-crawl) the index? (y/N): \").lower()\n",
        "        if choice == 'y':\n",
        "            engine.crawl()\n",
        "        else:\n",
        "            # Build semantic and BM25 indexes if not loaded.\n",
        "            if not engine.vector_store or not engine.bm25:\n",
        "                engine.build_indexes()\n",
        "    else:\n",
        "        engine.crawl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOS93c--B2eJ",
        "outputId": "aaafd238-d8b0-44ba-df35-bad5fcdfbb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vector store from disk.\n",
            "Loaded BM25 index from disk.\n",
            "Index found. Do you want to update (re-crawl) the index? (y/N): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "engine.interactive_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqVR_7VEDQro",
        "outputId": "dfbc2980-3eb7-4719-ebf4-c68c940a82c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter search query: how does similarity_search_with_relevance_score work?\n",
            "\n",
            "Semantic Search Results (found 5 results in 0.08s):\n",
            "1. https://python.langchain.com/docs/integrations/vectorstores/bageldb/\n",
            "Snippet: On this page BagelDB ( Open Vector Database for AI ), is like GitHub for AI data.\n",
            "It is a collaborative platform where users can create,\n",
            "share, and manage vector datasets. It can support private proje...\n",
            "\n",
            "2. https://python.langchain.com/docs/integrations/vectorstores/bageldb\n",
            "Snippet: On this page BagelDB ( Open Vector Database for AI ), is like GitHub for AI data.\n",
            "It is a collaborative platform where users can create,\n",
            "share, and manage vector datasets. It can support private proje...\n",
            "\n",
            "3. https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.SearchType.html\n",
            "Snippet: SearchType # class langchain.retrievers.multi_vector. SearchType ( value ) [source] # Enumerator of the types of search to perform. similarity = 'similarity' # Similarity search. similarity_score_thre...\n",
            "\n",
            "4. https://python.langchain.com/docs/integrations/vectorstores/bagel/\n",
            "Snippet: On this page Bagel ( Open Inference platform for AI ), is like GitHub for AI data.\n",
            "It is a collaborative platform where users can create,\n",
            "share, and manage Inference datasets. It can support private p...\n",
            "\n",
            "5. https://python.langchain.com/docs/integrations/vectorstores/bagel\n",
            "Snippet: On this page Bagel ( Open Inference platform for AI ), is like GitHub for AI data.\n",
            "It is a collaborative platform where users can create,\n",
            "share, and manage Inference datasets. It can support private p...\n",
            "\n",
            "BM25 Sparse Search Results:\n",
            "1. https://python.langchain.com/docs/how_to/output_parser_custom/ (Score: 8.35)\n",
            "Snippet: On this page In some situations you may want to implement a custom parser to structure the model output into a custom format. There are two ways to implement a custom parser: Using RunnableLambda or R...\n",
            "\n",
            "2. https://python.langchain.com/v0.2/docs/how_to/output_parser_custom/ (Score: 8.26)\n",
            "Snippet: This is documentation for LangChain v0.2 , which is no longer actively maintained. For the current stable version, see this version ( Latest ). On this page How to create a custom Output Parser In som...\n",
            "\n",
            "3. https://python.langchain.com/v0.1/docs/additional_resources/youtube/ (Score: 7.48)\n",
            "Snippet: This is documentation for LangChain v0.1 , which is no longer actively maintained. For the current stable version, see this version ( Latest ). On this page YouTube videos â›“ icon marks a new addition ...\n",
            "\n",
            "4. https://python.langchain.com/v0.1/docs/expression_language/streaming/ (Score: 5.23)\n",
            "Snippet: This is documentation for LangChain v0.1 , which is no longer actively maintained. For the current stable version, see this version ( Latest ). On this page Streaming With LangChain Streaming is criti...\n",
            "\n",
            "5. https://python.langchain.com/docs/how_to/streaming/ (Score: 4.89)\n",
            "Snippet: On this page Prerequisites This guide assumes familiarity with the following concepts: Chat models LangChain Expression Language Output parsers Streaming is critical in making applications based on LL...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uvd-_HVNW_4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}