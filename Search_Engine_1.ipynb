{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwtELydmPaSK+0rddHpWAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balnarendrasapa/search-engine/blob/main/Search_Engine_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "AuPkNVJU8l-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community faiss-cpu rank_bm25"
      ],
      "metadata": {
        "id": "wUTCrTM7-S23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7ddde3-91f3-49d6-cfa8-78a9282bb8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Index from the repository\n",
        "\n",
        "- This is a pre-built index from my repository\n",
        "- This step is optional.\n",
        "- If you want to build fresh index, Just don't run this step."
      ],
      "metadata": {
        "id": "8GoVEBiO8oae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/balnarendrasapa/search-engine/raw/refs/heads/main/built_index.zip"
      ],
      "metadata": {
        "id": "2Y4P8q3j7ytd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f13d7d-3047-45c9-abeb-ae386fce3c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-05 17:13:44--  https://github.com/balnarendrasapa/search-engine/raw/refs/heads/main/built_index.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/balnarendrasapa/search-engine/refs/heads/main/built_index.zip [following]\n",
            "--2025-04-05 17:13:45--  https://raw.githubusercontent.com/balnarendrasapa/search-engine/refs/heads/main/built_index.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3443323 (3.3M) [application/zip]\n",
            "Saving to: ‘built_index.zip’\n",
            "\n",
            "built_index.zip     100%[===================>]   3.28M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-04-05 17:13:45 (40.4 MB/s) - ‘built_index.zip’ saved [3443323/3443323]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip built_index.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuQsk1bS8c-s",
        "outputId": "324f62d3-d0aa-4be4-807a-00c346940e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  built_index.zip\n",
            "   creating: built_index/\n",
            "   creating: built_index/vector_store/\n",
            "  inflating: built_index/vector_store/index.pkl  \n",
            "  inflating: built_index/vector_store/index.faiss  \n",
            "  inflating: built_index/search_index.json  \n",
            "  inflating: built_index/bm25_index.pkl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawler Code\n",
        "\n",
        "- If you want the crawler to crawl more pages, set PAGES_TO_CRAWL value to your liking\n",
        "- TOP_K_RESULTS will show you top k searches.\n",
        "- You can also change the BASE_URL to your liking"
      ],
      "metadata": {
        "id": "mqKhE4gN86Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAGES_TO_CRAWL = 500\n",
        "TOP_K_RESULTS = 5\n",
        "BASE_URL = \"https://python.langchain.com/\"\n",
        "BASE_DIR = \"built_index\"\n",
        "EMBEDDINGS_MODEL = \"all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "id": "rFSo6wbnI07F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urldefrag\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import Hugging Face embeddings from LangChain and FAISS vector store\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Import BM25 from rank_bm25 for sparse searching\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    os.makedirs(BASE_DIR)\n",
        "\n",
        "class SearchEngine:\n",
        "    def __init__(self,\n",
        "                 base_url=\"https://python.langchain.com/\",\n",
        "                 index_file=BASE_DIR + \"/\"+ \"search_index.json\",\n",
        "                 vector_store_dir=BASE_DIR + \"/\" + \"vector_store\",\n",
        "                 bm25_index_file=BASE_DIR + \"/\" + \"bm25_index.pkl\"):\n",
        "        self.base_url = base_url\n",
        "        self.index_file = index_file\n",
        "        self.vector_store_dir = vector_store_dir\n",
        "        self.bm25_index_file = bm25_index_file\n",
        "        self.index = defaultdict(dict)\n",
        "        self.visited_urls = set()\n",
        "        self.urls_to_visit = [base_url]\n",
        "        self.load_index()\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        self.vector_store = None\n",
        "        self.bm25 = None      # BM25 object for sparse search\n",
        "        self.bm25_texts = []  # List of tokenized texts\n",
        "        self.url_order = []   # To maintain order corresponding to BM25 texts\n",
        "\n",
        "        # Load semantic vector store if available\n",
        "        if os.path.exists(self.vector_store_dir):\n",
        "            try:\n",
        "                self.vector_store = FAISS.load_local(\n",
        "                    self.vector_store_dir,\n",
        "                    self.embeddings,\n",
        "                    allow_dangerous_deserialization=True\n",
        "                )\n",
        "                print(\"Loaded vector store from disk.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load vector store: {e}\")\n",
        "\n",
        "        # Load BM25 index if available\n",
        "        self.load_bm25_index()\n",
        "\n",
        "    def load_index(self):\n",
        "        if os.path.exists(self.index_file):\n",
        "            with open(self.index_file, 'r') as f:\n",
        "                self.index = defaultdict(dict, json.load(f))\n",
        "\n",
        "    def save_index(self):\n",
        "        with open(self.index_file, 'w') as f:\n",
        "            json.dump(dict(self.index), f)\n",
        "\n",
        "    def load_bm25_index(self):\n",
        "        if os.path.exists(self.bm25_index_file):\n",
        "            try:\n",
        "                with open(self.bm25_index_file, 'rb') as f:\n",
        "                    self.bm25, self.bm25_texts, self.url_order = pickle.load(f)\n",
        "                print(\"Loaded BM25 index from disk.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load BM25 index: {e}\")\n",
        "\n",
        "    def save_bm25_index(self):\n",
        "        try:\n",
        "            with open(self.bm25_index_file, 'wb') as f:\n",
        "                pickle.dump((self.bm25, self.bm25_texts, self.url_order), f)\n",
        "            print(\"BM25 index saved to disk.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save BM25 index: {e}\")\n",
        "\n",
        "    def fetch_page(self, url):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_content(self, html):\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        # Remove unwanted tags\n",
        "        for elem in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
        "            elem.decompose()\n",
        "        main_content = soup.find('main') or soup.find('article') or soup\n",
        "        return main_content.get_text(separator=' ', strip=True)\n",
        "\n",
        "    def process_page(self, url, html):\n",
        "        content = self.extract_content(html)\n",
        "        self.index[url] = {\n",
        "            'content': content,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "\n",
        "    def find_links(self, html, base_url):\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            absolute_url = urljoin(base_url, href)\n",
        "            absolute_url, _ = urldefrag(absolute_url)\n",
        "            if absolute_url.startswith(self.base_url) and absolute_url not in self.visited_urls:\n",
        "                self.urls_to_visit.append(absolute_url)\n",
        "\n",
        "    def crawl(self, max_pages=PAGES_TO_CRAWL):\n",
        "        print(\"Starting indexing process...\")\n",
        "        pbar = tqdm(total=max_pages, desc=\"Pages Crawled\")\n",
        "        while self.urls_to_visit and len(self.visited_urls) < max_pages:\n",
        "            current_url = self.urls_to_visit.pop(0)\n",
        "            if current_url in self.visited_urls:\n",
        "                continue\n",
        "\n",
        "            html = self.fetch_page(current_url)\n",
        "            if html:\n",
        "                self.process_page(current_url, html)\n",
        "                self.find_links(html, current_url)\n",
        "                self.visited_urls.add(current_url)\n",
        "                pbar.update(1)\n",
        "        pbar.close()\n",
        "        self.save_index()\n",
        "        print(f\"Index updated. Total pages: {len(self.index)}\")\n",
        "        self.build_indexes()\n",
        "\n",
        "    def build_indexes(self):\n",
        "        # Build semantic vector store and BM25 index together.\n",
        "        print(\"Building semantic vector store and BM25 sparse index...\")\n",
        "        texts, metadatas = [], []\n",
        "        self.url_order = []  # reset BM25 order list\n",
        "        for url, data in self.index.items():\n",
        "            texts.append(data['content'])\n",
        "            metadatas.append({\"url\": url})\n",
        "            self.url_order.append(url)\n",
        "        # Build semantic index\n",
        "        self.vector_store = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)\n",
        "        self.vector_store.save_local(self.vector_store_dir)\n",
        "        print(\"Semantic vector store built and saved successfully.\")\n",
        "        # Build BM25 index\n",
        "        self.build_bm25_index(texts)\n",
        "\n",
        "    def build_bm25_index(self, texts):\n",
        "        print(\"Building BM25 sparse index...\")\n",
        "        # Tokenize each document (using a simple whitespace split)\n",
        "        self.bm25_texts = [text.lower().split() for text in texts]\n",
        "        self.bm25 = BM25Okapi(self.bm25_texts)\n",
        "        print(\"BM25 index built.\")\n",
        "        self.save_bm25_index()\n",
        "\n",
        "    def bm25_search(self, query, top_k=TOP_K_RESULTS):\n",
        "        if not self.bm25:\n",
        "            print(\"BM25 index is not built. Building BM25 index now...\")\n",
        "            texts = [data['content'] for data in self.index.values()]\n",
        "            self.build_bm25_index(texts)\n",
        "        query_tokens = query.lower().split()\n",
        "        scores = self.bm25.get_scores(query_tokens)\n",
        "        # Get indices of the top k scores\n",
        "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            url = self.url_order[idx] if idx < len(self.url_order) else \"N/A\"\n",
        "            results.append({\n",
        "                \"url\": url,\n",
        "                \"score\": scores[idx],\n",
        "                \"snippet\": self.index[url]['content'][:200] if url in self.index else \"\"\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def semantic_search(self, query, top_k=TOP_K_RESULTS):\n",
        "        if not self.vector_store:\n",
        "            print(\"Semantic vector store is not built. Building now...\")\n",
        "            self.build_indexes()\n",
        "        results = self.vector_store.similarity_search(query, k=top_k)\n",
        "        sem_results = []\n",
        "        for result in results:\n",
        "            url = result.metadata.get(\"url\", \"N/A\")\n",
        "            sem_results.append({\n",
        "                \"url\": url,\n",
        "                \"score\": None,  # semantic search does not return a raw BM25-like score\n",
        "                \"snippet\": result.page_content[:200]\n",
        "            })\n",
        "        return sem_results\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"\n",
        "        method can be 'semantic', 'bm25', or 'combined'\n",
        "        \"\"\"\n",
        "        sem_results = self.semantic_search(query, top_k=top_k)\n",
        "        bm25_results = self.bm25_search(query, top_k=top_k)\n",
        "        return {\"semantic\": sem_results, \"bm25\": bm25_results}\n",
        "\n",
        "    def interactive_search(self):\n",
        "        query = input(\"\\nEnter search query: \").strip()\n",
        "        start_time = time.time()\n",
        "        results = self.search(query)\n",
        "        search_time = time.time() - start_time\n",
        "        print(f\"\\nSemantic Search Results (found {len(results['semantic'])} results in {search_time:.2f}s):\")\n",
        "        for i, res in enumerate(results['semantic'], 1):\n",
        "            print(f\"{i}. {res['url']}\")\n",
        "            print(f\"Snippet: {res['snippet']}...\\n\")\n",
        "        print(f\"BM25 Sparse Search Results:\")\n",
        "        for i, res in enumerate(results['bm25'], 1):\n",
        "            print(f\"{i}. {res['url']} (Score: {res['score']:.2f})\")\n",
        "            print(f\"Snippet: {res['snippet']}...\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    engine = SearchEngine(BASE_URL)\n",
        "\n",
        "    # If the index file exists, ask whether to update/re-crawl the index.\n",
        "    if os.path.exists(engine.index_file) and engine.index:\n",
        "        choice = input(\"Index found. Do you want to update (re-crawl) the index? (y/N): \").lower()\n",
        "        if choice == 'y':\n",
        "            engine.crawl()\n",
        "        else:\n",
        "            # Build semantic and BM25 indexes if not loaded.\n",
        "            if not engine.vector_store or not engine.bm25:\n",
        "                engine.build_indexes()\n",
        "    else:\n",
        "        engine.crawl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667,
          "referenced_widgets": [
            "bf7091e4dffe4e92b35ee1a4758410b7",
            "00ecb62428f640b8b31bde25267d6ea8",
            "8741734398cb4d709c86bfcd168671fc",
            "e292e67827a04e038765c34d6e7ca573",
            "d93701d3659f41c1a710bd66e9f2f15a",
            "79c746c8f1554e939e15736f063da2e5",
            "c3b94aadf0724889a31c5b238b01489b",
            "9afc30669e604b57aaaaca2e13513f0e",
            "778065a03cc5491cac599972266d585d",
            "d712e883d0c5415d8f9a7467ce43e3aa",
            "700f4d84559448e29d49a594f2ba920c",
            "23958018ce134a36b1c836656106a087",
            "221bec449cb349b9919d86e783379154",
            "10b68d49effc4caeaf90582829e7866d",
            "47ef3efda945483e84e9b874985d2db9",
            "7d8e8851cf424d0f927ab22442564616",
            "89fdc991fef34281bb0d16172224453c",
            "eb8f383a57dd41cf9af299534ccd8648",
            "41892194b8004a7599d1be5eaf12f912",
            "60485ba4fde94495a56b73d5da0b3c68",
            "340e8b473e904fec8bed2d71174dfee8",
            "9b8b3a74981e45d4a7983d02727dbcf0",
            "b6e19d2690bb4fa894d5c54d7c5a7cfc",
            "b6371df74dc0489fa4aa1e96387a5969",
            "f94a412cbd1f499296965dbf2f19f084",
            "bf1242c4e345407e8f95a70ce617c59a",
            "28b37c743bd0433c81c1074c4976c235",
            "9309dc23ecf74b75b4a527bcf69eaf8a",
            "b77fe8cfc43c451794dca5bc2d839be8",
            "b7a1593b7fcf4d18b506cf8ec3448e81",
            "50ac86895d0d40a9bb305f9d07f449f4",
            "2a8a21f4a66a4a1ab49e0b55413a265e",
            "df3464b294064203aa23e4f898f1dd8c",
            "124629abd26d402ab9526fc0e16782b8",
            "578399a723b6469fbf92df61fd6e63b4",
            "ebd3c38f25ff47b3983896be28c49566",
            "846ee33112314e3ab8689e198e48dc0b",
            "1dcfcd128fe44b64a619b0b049474463",
            "f312b454f3164775a63f5e068d21fd58",
            "a0d512ef8af9491baa0f75c2e9302862",
            "488282fa450a4971a25ba16d3fd554d9",
            "5364e8c0e502474681343cde23a19c17",
            "64eba8ebbbbb49d0a585a1a7017f3a70",
            "b0f02be86e8040b8a133726c421d976c",
            "56c915468412421a80f8cbaad99fa969",
            "d91530501efe4e87bcafc6eb499be04e",
            "bc25c02318d849b2b82bee25225113e9",
            "d2e0512ec17e44e9ab4e4e60a69a4000",
            "d8e2edf19c6841e9a2d6c6a1cf679b82",
            "c07cf38d0b3d45719c0c373a63859701",
            "07696f51946d4d52ae49c3d2e39c2672",
            "631fa827360645b99ce7cd76f00614c0",
            "4a2792b0111c4ecab348fdab796af73f",
            "388d2770bdee44ff8e8ff2b580feba6d",
            "b4dfab32e853481a886feda6b8225833",
            "e59d27cfdf3d4108b1e3de572caa5eac",
            "3a6a60ed7c98446fad8b3495ce96e984",
            "cac15a24e5244601ac9a0447e0fa4f90",
            "06d8786caa9c4973b491050c3966dfd6",
            "a225fda5bad5423b9890ded2471bfa4c",
            "0b435ca2a9064028b1b77374afe48c80",
            "9bd9dbc1fdeb4c0e958bad6ffad90c18",
            "49825b82fac346ba8082566118d6b77a",
            "44b4f3c7ae2e4343b0f46787d88c0291",
            "129c225470e2411e97bfded77f557aaf",
            "b37cb36e512f45ce9e708ac7177f3689",
            "aa6e5e4dfce54b6ca05c8eebeaee1848",
            "7e3140b3b68846e9989b5de71d5bb72c",
            "8426e793d7434dccb6e450934455fc53",
            "1c34ada9c6784597853c0e13048bad50",
            "a28410351a1d48bdbdfcf1cc39e3386d",
            "9fb954e7488d488c9107ae5844052cd4",
            "e1f9688276634d8bba9027056138381e",
            "97ed19a318dc4201bf7f634420c0db9d",
            "33d2fc3e0dd94e29bd239a6cd945bad6",
            "6ab76a91aa5949c8ac302af08d62c963",
            "78831cf22fc5458db35248eaad62a808",
            "ba252f00e7384b3f8b908f1a15e6bc97",
            "0cc18272d19b41f0953371f660afc7af",
            "abc12a4c8e184d1184816d1219ff264f",
            "7bf8de151e6244e89ef1f7b4a311542d",
            "b3d52e54b6494384b9944a7fbf1db902",
            "27814b522ca64a4bb64a78d280080962",
            "3bdf0aa904b14b8eae140d0e9930d7f4",
            "b495ef5a30f94d85b4fbd806a20614b1",
            "18d2d83d6b8140138873bd1ff1e7e682",
            "29d3b1c5da574fc39b1dff2ac7e4d99b",
            "27ee31494ee749b38bdacebeca3b4035",
            "cb38608a9d494c8a8d14e6296b945084",
            "cb00674017c94c80a22725d78ad39647",
            "7c0b6eef18b3475f8ad02356f48d10ea",
            "f4319adbcd6a409d89b153f54333f35f",
            "b00de92012014ae2a486d77af7486a57",
            "52d98563c0bb412c9e4f513fc5491385",
            "985ec4e806894630a0412cb490e4a537",
            "bbfe0506ab0241f7aba3c35ceb0717e9",
            "6041bc3e5eb1411caaef1d57ab69069b",
            "9140f4c0e47648bca390ec1cb18bb2a1",
            "8442a03cae4448518704ca81ddbe721e",
            "97ba0380fae744d6b2e2e7c3d94ab49b",
            "dfbdb62edf004393b1029db9614ba18b",
            "2ca6bdd735fa4e89afd97f8f8aa872a3",
            "9db8601592ae41f4a0b7df89d783d738",
            "9fc93d8a81e14cc5a903fedc4d1605e9",
            "90c9671137e94fabb9baf2cf1b4e0767",
            "12c8b2f6f6714c8a810b6955a125d064",
            "d2b535fb432e400190ea43e4020f4973",
            "cf02d8aaa4e549dcae0a6b1f10253e0f",
            "486501c7d27c4ed2bd82f376b472fae5",
            "9b73b818a9214425a7ddd1c2731d6f50",
            "66bdd77e214c47ffa510fbaaeee1fb59",
            "1d755f04186d4253b9d6f29d284b6601",
            "9bc3aae80cc64317860439b397ee9251",
            "11f4604fe38742299cddaf2b1e969b36",
            "6b3f66adf52f40c3bb3bfbae32011178",
            "b821d7baaf064757ad827b1ad932d63c",
            "cc346c2a158e4e2aadae66279e8de7ad",
            "2b5b8231118a4a649b6ab64b1f16abcf",
            "cf31388cd3204199a129c245c2ed7755",
            "05ad15d3009b44388840eceee45630b3",
            "6da70b60201c423e9bd727afb1a85e94"
          ]
        },
        "id": "iOS93c--B2eJ",
        "outputId": "e8ed67c9-35a2-4f49-d59f-5f3fbae5f497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-92f60946a770>:35: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  self.embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf7091e4dffe4e92b35ee1a4758410b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23958018ce134a36b1c836656106a087"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6e19d2690bb4fa894d5c54d7c5a7cfc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "124629abd26d402ab9526fc0e16782b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56c915468412421a80f8cbaad99fa969"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e59d27cfdf3d4108b1e3de572caa5eac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa6e5e4dfce54b6ca05c8eebeaee1848"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba252f00e7384b3f8b908f1a15e6bc97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb38608a9d494c8a8d14e6296b945084"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97ba0380fae744d6b2e2e7c3d94ab49b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66bdd77e214c47ffa510fbaaeee1fb59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting indexing process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pages Crawled: 100%|██████████| 500/500 [02:48<00:00,  2.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index updated. Total pages: 500\n",
            "Building semantic vector store and BM25 sparse index...\n",
            "Semantic vector store built and saved successfully.\n",
            "Building BM25 sparse index...\n",
            "BM25 index built.\n",
            "BM25 index saved to disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search the Index.\n",
        "\n",
        "- There are two kinds of searches\n",
        "- One is semantic searching using vector embeddings\n",
        "- Another is sparse searching using bm25."
      ],
      "metadata": {
        "id": "0TtHXugo89xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engine.interactive_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqVR_7VEDQro",
        "outputId": "77676956-a77b-4a4b-f84c-f05a9a982a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter search query: what is embeddings?\n",
            "\n",
            "Semantic Search Results (found 5 results in 0.02s):\n",
            "1. https://python.langchain.com/docs/how_to/embed_text/\n",
            "Snippet: On this page info Head to Integrations for documentation on built-in integrations with text embedding model providers. The Embeddings class is a class designed for interfacing with text embedding mode...\n",
            "\n",
            "2. https://python.langchain.com/docs/how_to/custom_embeddings/\n",
            "Snippet: On this page LangChain is integrated with many 3rd party embedding models . In this guide we'll show you how to create a custom Embedding class, in case a built-in one does not already exist. Embeddin...\n",
            "\n",
            "3. https://python.langchain.com/docs/concepts/embedding_models/\n",
            "Snippet: On this page Prerequisites Documents Note This conceptual overview focuses on text-based embedding models. Embedding models can also be multimodal though such models are not currently supported by Lan...\n",
            "\n",
            "4. https://python.langchain.com/docs/how_to/caching_embeddings/\n",
            "Snippet: On this page Embeddings can be stored or temporarily cached to avoid needing to recompute them. Caching embeddings can be done using a CacheBackedEmbeddings . The cache backed embedder is a wrapper ar...\n",
            "\n",
            "5. https://python.langchain.com/docs/integrations/providers/infinity/\n",
            "Snippet: On this page Infinity allows the creation of text embeddings. Text Embedding Model ​ There exists an infinity Embedding model, which you can access with from langchain_community . embeddings import In...\n",
            "\n",
            "BM25 Sparse Search Results:\n",
            "1. https://python.langchain.com/docs/integrations/providers/helicone/ (Score: 5.04)\n",
            "Snippet: On this page This page covers how to use the Helicone ecosystem within LangChain. What is Helicone? ​ Helicone is an open-source observability platform that proxies your OpenAI traffic and provides yo...\n",
            "\n",
            "2. https://python.langchain.com/docs/concepts/few_shot_prompting/ (Score: 5.02)\n",
            "Snippet: On this page Prerequisites Chat models Overview ​ One of the most effective ways to improve model performance is to give a model examples of\n",
            "what you want it to do. The technique of adding example inp...\n",
            "\n",
            "3. https://python.langchain.com/docs/versions/v0_2/overview/ (Score: 5.01)\n",
            "Snippet: On this page What’s new in LangChain? ​ The following features have been added during the development of 0.1.x: Better streaming support via the Event Streaming API . Standardized tool calling support...\n",
            "\n",
            "4. https://python.langchain.com/docs/integrations/providers/metal/ (Score: 4.98)\n",
            "Snippet: On this page This page covers how to use Metal within LangChain. What is Metal? ​ Metal is a  managed retrieval & memory platform built for production. Easily index your data into Metal and run semant...\n",
            "\n",
            "5. https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/ (Score: 4.95)\n",
            "Snippet: On this page ConversationBufferMemory and ConversationStringBufferMemory were used to keep track of a conversation between a human and an ai asstistant without any additional processing. note The Conv...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-QRpOh57ZwV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}